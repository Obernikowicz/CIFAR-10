{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59a9780d",
      "metadata": {
        "id": "59a9780d"
      },
      "source": [
        "### Zadanie domowe 3\n",
        "\n",
        "Proszę skonstruować klasyfikator oparty o głęboką sieć neuronową dla danych CIFAR-10. \n",
        "\n",
        "Do konstrukcji klasyfikatora należy użyć zbioru treningowego danych CIFAR-10 a zbioru testowego CIFAR-10 jako zbioru walidacyjnego do oceny dokładności aktualnego modelu po każdej epoce porcedury trenowania. \n",
        "\n",
        "Rozwiązaniem powinien być plik .ipynb z kodem w Pythonie wczytującym dane, dokonującym ewentualnych przekształceń danych, trenującym opisany klasyfikator i generującym wykresy dokładności modelu zarówno dla danych treningowych jak i walidacyjnych uzyskanych po każdej epoce trenowania. Procedura trenowania powinna wykorzystywać \"patient early stopping\" i zapisywać parametry najlepszego modelu do pliku. Po wytrenowaniu, zapisany model powinien zostać wczytany z zapisanego pliku i zastosowany do danych walidacyjnych celem obliczenia i wypisania dokładności osiągniętej dla tych danych.  \n",
        "\n",
        "Wytrenowany model powinnien osiągać dokładność na danych walidacyjnych wynoszącą co najmniej 70%.\n",
        "\n",
        "***Wskazówki:*** \n",
        "- Zastanowić się jaki powinien być \"input shape\".\n",
        "- Model powinien być głęboką siecią neuronową, jednak jej architektura jest pozostawiona Państwa inwencji.\n",
        "\n",
        "***Termin oddania***: 9 czerwca 2023 (za 3 tygodnie of daty umieszczenia na platformie Moodle)\n",
        "\n",
        "**Propozycja do rozważenia**\n",
        "\n",
        "Jeżeli Państwo się zgodzą, możemy zorganizować mały konkurs. Osoby chcące wziąć w nim udział będą dodatkowo proszone o zachowanie pliku .hd5 z jednym wytrenowanym modelem. Po ustaleniu jakiegoś dodatkowego terminu pasującego uszestnikom konkursu, komisyjnie wczytamy Państwa zapisane modele i uruchomimy na zbiorze testowym CIFAR-10. Zwycięskim modelem będzie ten, który osiągnie maksymalną dokładność na tym zbiorze danych. Nagrodą dla zwycięzcy(-ów) będą 3 dodatkowe punkty do oceny końcowej. Szczegóły ustalimy na następnych zajęciach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "764794ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "764794ce",
        "outputId": "9e491b5f-1eb1-4113-de91-962c39152910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "152Ca6WVBcJC"
      },
      "id": "152Ca6WVBcJC",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "n2Y3rq-FBokG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c39fd-9fc2-40e8-d63b-8b481e6d671d"
      },
      "id": "n2Y3rq-FBokG",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        ")"
      ],
      "metadata": {
        "id": "29cpSzCZ0OSK"
      },
      "id": "29cpSzCZ0OSK",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = x_train / 255, x_test / 255\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "VGEjQj5jCJV6"
      },
      "id": "VGEjQj5jCJV6",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "Q480E08v0jUY"
      },
      "id": "Q480E08v0jUY",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(32, 32, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "TX-F-EQUClEI"
      },
      "id": "TX-F-EQUClEI",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOsX2zSdEdXP",
        "outputId": "92dae1f3-d2c3-4d92-f57c-ef3b8bf39624"
      },
      "id": "qOsX2zSdEdXP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 8, 8, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 8, 8, 256)        1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2097664   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,380,554\n",
            "Trainable params: 3,378,762\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
        "\n",
        "steps_per_epoch = len(x_train) // batch_size"
      ],
      "metadata": {
        "id": "D4xzhhQm0y-H"
      },
      "id": "D4xzhhQm0y-H",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    'best_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=500, \n",
        "                    validation_data=(x_test, y_test), callbacks=[early_stopping, checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHbStGASEruL",
        "outputId": "e8528696-43b2-4a88-cee9-54212aece14e"
      },
      "id": "ZHbStGASEruL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.8879 - accuracy: 0.3408\n",
            "Epoch 1: val_accuracy improved from -inf to 0.18250, saving model to best_model.h5\n",
            "390/390 [==============================] - 48s 87ms/step - loss: 1.8879 - accuracy: 0.3408 - val_loss: 2.4081 - val_accuracy: 0.1825\n",
            "Epoch 2/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.3775 - accuracy: 0.5112\n",
            "Epoch 2: val_accuracy improved from 0.18250 to 0.57680, saving model to best_model.h5\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 1.3775 - accuracy: 0.5112 - val_loss: 1.2633 - val_accuracy: 0.5768\n",
            "Epoch 3/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.1523 - accuracy: 0.6058\n",
            "Epoch 3: val_accuracy improved from 0.57680 to 0.65740, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 1.1523 - accuracy: 0.6058 - val_loss: 1.0066 - val_accuracy: 0.6574\n",
            "Epoch 4/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 1.0020 - accuracy: 0.6630\n",
            "Epoch 4: val_accuracy improved from 0.65740 to 0.71560, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 1.0020 - accuracy: 0.6630 - val_loss: 0.8986 - val_accuracy: 0.7156\n",
            "Epoch 5/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.7056\n",
            "Epoch 5: val_accuracy did not improve from 0.71560\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.8916 - accuracy: 0.7056 - val_loss: 0.9491 - val_accuracy: 0.6932\n",
            "Epoch 6/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.8113 - accuracy: 0.7313\n",
            "Epoch 6: val_accuracy did not improve from 0.71560\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.8113 - accuracy: 0.7313 - val_loss: 0.9626 - val_accuracy: 0.6828\n",
            "Epoch 7/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.7578\n",
            "Epoch 7: val_accuracy improved from 0.71560 to 0.77930, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.7394 - accuracy: 0.7578 - val_loss: 0.6945 - val_accuracy: 0.7793\n",
            "Epoch 8/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.7798\n",
            "Epoch 8: val_accuracy improved from 0.77930 to 0.78610, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.6828 - accuracy: 0.7798 - val_loss: 0.6435 - val_accuracy: 0.7861\n",
            "Epoch 9/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6508 - accuracy: 0.7891\n",
            "Epoch 9: val_accuracy did not improve from 0.78610\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.6508 - accuracy: 0.7891 - val_loss: 0.6663 - val_accuracy: 0.7790\n",
            "Epoch 10/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.6142 - accuracy: 0.8051\n",
            "Epoch 10: val_accuracy did not improve from 0.78610\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.6142 - accuracy: 0.8051 - val_loss: 0.6879 - val_accuracy: 0.7785\n",
            "Epoch 11/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5780 - accuracy: 0.8126\n",
            "Epoch 11: val_accuracy improved from 0.78610 to 0.79600, saving model to best_model.h5\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.5780 - accuracy: 0.8126 - val_loss: 0.5984 - val_accuracy: 0.7960\n",
            "Epoch 12/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5492 - accuracy: 0.8244\n",
            "Epoch 12: val_accuracy improved from 0.79600 to 0.82460, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.5492 - accuracy: 0.8244 - val_loss: 0.5332 - val_accuracy: 0.8246\n",
            "Epoch 13/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.8301\n",
            "Epoch 13: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.5316 - accuracy: 0.8301 - val_loss: 0.5981 - val_accuracy: 0.7952\n",
            "Epoch 14/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.8374\n",
            "Epoch 14: val_accuracy improved from 0.82460 to 0.82650, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.5022 - accuracy: 0.8374 - val_loss: 0.5207 - val_accuracy: 0.8265\n",
            "Epoch 15/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4799 - accuracy: 0.8466\n",
            "Epoch 15: val_accuracy improved from 0.82650 to 0.84770, saving model to best_model.h5\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.4799 - accuracy: 0.8466 - val_loss: 0.4837 - val_accuracy: 0.8477\n",
            "Epoch 16/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8533\n",
            "Epoch 16: val_accuracy did not improve from 0.84770\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.4602 - accuracy: 0.8533 - val_loss: 0.4896 - val_accuracy: 0.8384\n",
            "Epoch 17/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8574\n",
            "Epoch 17: val_accuracy did not improve from 0.84770\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.4440 - accuracy: 0.8574 - val_loss: 0.4987 - val_accuracy: 0.8374\n",
            "Epoch 18/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.8618\n",
            "Epoch 18: val_accuracy did not improve from 0.84770\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.4308 - accuracy: 0.8618 - val_loss: 0.4723 - val_accuracy: 0.8470\n",
            "Epoch 19/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4153 - accuracy: 0.8676\n",
            "Epoch 19: val_accuracy improved from 0.84770 to 0.84790, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.4153 - accuracy: 0.8676 - val_loss: 0.4838 - val_accuracy: 0.8479\n",
            "Epoch 20/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8711\n",
            "Epoch 20: val_accuracy did not improve from 0.84790\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.4010 - accuracy: 0.8711 - val_loss: 0.4928 - val_accuracy: 0.8448\n",
            "Epoch 21/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.8722\n",
            "Epoch 21: val_accuracy did not improve from 0.84790\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3990 - accuracy: 0.8722 - val_loss: 0.5758 - val_accuracy: 0.8136\n",
            "Epoch 22/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.8777\n",
            "Epoch 22: val_accuracy improved from 0.84790 to 0.85500, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3833 - accuracy: 0.8777 - val_loss: 0.4439 - val_accuracy: 0.8550\n",
            "Epoch 23/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3714 - accuracy: 0.8819\n",
            "Epoch 23: val_accuracy improved from 0.85500 to 0.85950, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.3714 - accuracy: 0.8819 - val_loss: 0.4427 - val_accuracy: 0.8595\n",
            "Epoch 24/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.8867\n",
            "Epoch 24: val_accuracy did not improve from 0.85950\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.3572 - accuracy: 0.8867 - val_loss: 0.4451 - val_accuracy: 0.8579\n",
            "Epoch 25/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3518 - accuracy: 0.8879\n",
            "Epoch 25: val_accuracy did not improve from 0.85950\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3518 - accuracy: 0.8879 - val_loss: 0.5061 - val_accuracy: 0.8395\n",
            "Epoch 26/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8931\n",
            "Epoch 26: val_accuracy did not improve from 0.85950\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.3398 - accuracy: 0.8931 - val_loss: 0.4968 - val_accuracy: 0.8373\n",
            "Epoch 27/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8934\n",
            "Epoch 27: val_accuracy improved from 0.85950 to 0.86020, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.3336 - accuracy: 0.8934 - val_loss: 0.4214 - val_accuracy: 0.8602\n",
            "Epoch 28/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.8972\n",
            "Epoch 28: val_accuracy did not improve from 0.86020\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.3236 - accuracy: 0.8972 - val_loss: 0.4919 - val_accuracy: 0.8507\n",
            "Epoch 29/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8982\n",
            "Epoch 29: val_accuracy improved from 0.86020 to 0.87110, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.3208 - accuracy: 0.8982 - val_loss: 0.4071 - val_accuracy: 0.8711\n",
            "Epoch 30/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9010\n",
            "Epoch 30: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.3080 - accuracy: 0.9010 - val_loss: 0.4483 - val_accuracy: 0.8613\n",
            "Epoch 31/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9036\n",
            "Epoch 31: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.3054 - accuracy: 0.9036 - val_loss: 0.4801 - val_accuracy: 0.8438\n",
            "Epoch 32/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9070\n",
            "Epoch 32: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.2877 - accuracy: 0.9070 - val_loss: 0.4919 - val_accuracy: 0.8495\n",
            "Epoch 33/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9103\n",
            "Epoch 33: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2873 - accuracy: 0.9103 - val_loss: 0.4751 - val_accuracy: 0.8482\n",
            "Epoch 34/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9118\n",
            "Epoch 34: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.2733 - accuracy: 0.9118 - val_loss: 0.4983 - val_accuracy: 0.8628\n",
            "Epoch 35/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9110\n",
            "Epoch 35: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.2789 - accuracy: 0.9110 - val_loss: 0.4852 - val_accuracy: 0.8521\n",
            "Epoch 36/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.9127\n",
            "Epoch 36: val_accuracy did not improve from 0.87110\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2776 - accuracy: 0.9127 - val_loss: 0.4296 - val_accuracy: 0.8678\n",
            "Epoch 37/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9160\n",
            "Epoch 37: val_accuracy improved from 0.87110 to 0.87740, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2627 - accuracy: 0.9160 - val_loss: 0.3952 - val_accuracy: 0.8774\n",
            "Epoch 38/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2677 - accuracy: 0.9154\n",
            "Epoch 38: val_accuracy did not improve from 0.87740\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.2677 - accuracy: 0.9154 - val_loss: 0.4622 - val_accuracy: 0.8552\n",
            "Epoch 39/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9201\n",
            "Epoch 39: val_accuracy did not improve from 0.87740\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.2549 - accuracy: 0.9201 - val_loss: 0.4523 - val_accuracy: 0.8616\n",
            "Epoch 40/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9207\n",
            "Epoch 40: val_accuracy did not improve from 0.87740\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.2488 - accuracy: 0.9207 - val_loss: 0.4310 - val_accuracy: 0.8574\n",
            "Epoch 41/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.9222\n",
            "Epoch 41: val_accuracy did not improve from 0.87740\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2416 - accuracy: 0.9222 - val_loss: 0.4151 - val_accuracy: 0.8736\n",
            "Epoch 42/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9234\n",
            "Epoch 42: val_accuracy did not improve from 0.87740\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.2401 - accuracy: 0.9234 - val_loss: 0.4260 - val_accuracy: 0.8703\n",
            "Epoch 43/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.9285\n",
            "Epoch 43: val_accuracy improved from 0.87740 to 0.87890, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.2296 - accuracy: 0.9285 - val_loss: 0.3843 - val_accuracy: 0.8789\n",
            "Epoch 44/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9250\n",
            "Epoch 44: val_accuracy did not improve from 0.87890\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.2392 - accuracy: 0.9250 - val_loss: 0.4174 - val_accuracy: 0.8601\n",
            "Epoch 45/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9272\n",
            "Epoch 45: val_accuracy improved from 0.87890 to 0.88850, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2284 - accuracy: 0.9272 - val_loss: 0.3810 - val_accuracy: 0.8885\n",
            "Epoch 46/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9290\n",
            "Epoch 46: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.2254 - accuracy: 0.9290 - val_loss: 0.4122 - val_accuracy: 0.8753\n",
            "Epoch 47/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9295\n",
            "Epoch 47: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.2185 - accuracy: 0.9295 - val_loss: 0.4138 - val_accuracy: 0.8826\n",
            "Epoch 48/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9296\n",
            "Epoch 48: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.2235 - accuracy: 0.9296 - val_loss: 0.4142 - val_accuracy: 0.8727\n",
            "Epoch 49/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9310\n",
            "Epoch 49: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.2128 - accuracy: 0.9310 - val_loss: 0.3721 - val_accuracy: 0.8839\n",
            "Epoch 50/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9346\n",
            "Epoch 50: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.2077 - accuracy: 0.9346 - val_loss: 0.3723 - val_accuracy: 0.8838\n",
            "Epoch 51/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9338\n",
            "Epoch 51: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.2033 - accuracy: 0.9338 - val_loss: 0.4253 - val_accuracy: 0.8811\n",
            "Epoch 52/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2000 - accuracy: 0.9362\n",
            "Epoch 52: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.2000 - accuracy: 0.9362 - val_loss: 0.4069 - val_accuracy: 0.8831\n",
            "Epoch 53/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9350\n",
            "Epoch 53: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.2053 - accuracy: 0.9350 - val_loss: 0.4267 - val_accuracy: 0.8756\n",
            "Epoch 54/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9391\n",
            "Epoch 54: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1977 - accuracy: 0.9391 - val_loss: 0.4277 - val_accuracy: 0.8760\n",
            "Epoch 55/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9378\n",
            "Epoch 55: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 30s 77ms/step - loss: 0.1952 - accuracy: 0.9378 - val_loss: 0.4878 - val_accuracy: 0.8610\n",
            "Epoch 56/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9408\n",
            "Epoch 56: val_accuracy did not improve from 0.88850\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1915 - accuracy: 0.9408 - val_loss: 0.4625 - val_accuracy: 0.8672\n",
            "Epoch 57/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9401\n",
            "Epoch 57: val_accuracy improved from 0.88850 to 0.89350, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1853 - accuracy: 0.9401 - val_loss: 0.3613 - val_accuracy: 0.8935\n",
            "Epoch 58/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9433\n",
            "Epoch 58: val_accuracy did not improve from 0.89350\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1794 - accuracy: 0.9433 - val_loss: 0.4125 - val_accuracy: 0.8803\n",
            "Epoch 59/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9422\n",
            "Epoch 59: val_accuracy did not improve from 0.89350\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1820 - accuracy: 0.9422 - val_loss: 0.4122 - val_accuracy: 0.8806\n",
            "Epoch 60/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9434\n",
            "Epoch 60: val_accuracy did not improve from 0.89350\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1790 - accuracy: 0.9434 - val_loss: 0.4857 - val_accuracy: 0.8705\n",
            "Epoch 61/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9439\n",
            "Epoch 61: val_accuracy did not improve from 0.89350\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1782 - accuracy: 0.9439 - val_loss: 0.4570 - val_accuracy: 0.8623\n",
            "Epoch 62/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9441\n",
            "Epoch 62: val_accuracy did not improve from 0.89350\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1736 - accuracy: 0.9441 - val_loss: 0.3803 - val_accuracy: 0.8934\n",
            "Epoch 63/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9443\n",
            "Epoch 63: val_accuracy improved from 0.89350 to 0.89500, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.1791 - accuracy: 0.9443 - val_loss: 0.3584 - val_accuracy: 0.8950\n",
            "Epoch 64/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9449\n",
            "Epoch 64: val_accuracy did not improve from 0.89500\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1748 - accuracy: 0.9449 - val_loss: 0.4287 - val_accuracy: 0.8798\n",
            "Epoch 65/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9475\n",
            "Epoch 65: val_accuracy did not improve from 0.89500\n",
            "390/390 [==============================] - 31s 78ms/step - loss: 0.1631 - accuracy: 0.9475 - val_loss: 0.5071 - val_accuracy: 0.8417\n",
            "Epoch 66/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9484\n",
            "Epoch 66: val_accuracy did not improve from 0.89500\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.1648 - accuracy: 0.9484 - val_loss: 0.4236 - val_accuracy: 0.8818\n",
            "Epoch 67/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9472\n",
            "Epoch 67: val_accuracy did not improve from 0.89500\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1689 - accuracy: 0.9472 - val_loss: 0.4144 - val_accuracy: 0.8813\n",
            "Epoch 68/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9488\n",
            "Epoch 68: val_accuracy improved from 0.89500 to 0.89660, saving model to best_model.h5\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.1625 - accuracy: 0.9488 - val_loss: 0.3530 - val_accuracy: 0.8966\n",
            "Epoch 69/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1632 - accuracy: 0.9489\n",
            "Epoch 69: val_accuracy did not improve from 0.89660\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1632 - accuracy: 0.9489 - val_loss: 0.4811 - val_accuracy: 0.8678\n",
            "Epoch 70/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9509\n",
            "Epoch 70: val_accuracy did not improve from 0.89660\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.1531 - accuracy: 0.9509 - val_loss: 0.3583 - val_accuracy: 0.8965\n",
            "Epoch 71/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9519\n",
            "Epoch 71: val_accuracy did not improve from 0.89660\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.1490 - accuracy: 0.9519 - val_loss: 0.4181 - val_accuracy: 0.8924\n",
            "Epoch 72/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9512\n",
            "Epoch 72: val_accuracy improved from 0.89660 to 0.89670, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1542 - accuracy: 0.9512 - val_loss: 0.3799 - val_accuracy: 0.8967\n",
            "Epoch 73/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9532\n",
            "Epoch 73: val_accuracy improved from 0.89670 to 0.89950, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1500 - accuracy: 0.9532 - val_loss: 0.3645 - val_accuracy: 0.8995\n",
            "Epoch 74/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9514\n",
            "Epoch 74: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1533 - accuracy: 0.9514 - val_loss: 0.4347 - val_accuracy: 0.8793\n",
            "Epoch 75/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9537\n",
            "Epoch 75: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1459 - accuracy: 0.9537 - val_loss: 0.4315 - val_accuracy: 0.8881\n",
            "Epoch 76/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9548\n",
            "Epoch 76: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1439 - accuracy: 0.9548 - val_loss: 0.4392 - val_accuracy: 0.8830\n",
            "Epoch 77/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9527\n",
            "Epoch 77: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1508 - accuracy: 0.9527 - val_loss: 0.3677 - val_accuracy: 0.8909\n",
            "Epoch 78/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9576\n",
            "Epoch 78: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1349 - accuracy: 0.9576 - val_loss: 0.3962 - val_accuracy: 0.8838\n",
            "Epoch 79/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9548\n",
            "Epoch 79: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1432 - accuracy: 0.9548 - val_loss: 0.4232 - val_accuracy: 0.8914\n",
            "Epoch 80/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9563\n",
            "Epoch 80: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1411 - accuracy: 0.9563 - val_loss: 0.4023 - val_accuracy: 0.8895\n",
            "Epoch 81/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9562\n",
            "Epoch 81: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1405 - accuracy: 0.9562 - val_loss: 0.3877 - val_accuracy: 0.8929\n",
            "Epoch 82/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9558\n",
            "Epoch 82: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 30s 78ms/step - loss: 0.1375 - accuracy: 0.9558 - val_loss: 0.3710 - val_accuracy: 0.8934\n",
            "Epoch 83/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9556\n",
            "Epoch 83: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1444 - accuracy: 0.9556 - val_loss: 0.3716 - val_accuracy: 0.8885\n",
            "Epoch 84/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9563\n",
            "Epoch 84: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1362 - accuracy: 0.9563 - val_loss: 0.3731 - val_accuracy: 0.8985\n",
            "Epoch 85/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.9593\n",
            "Epoch 85: val_accuracy did not improve from 0.89950\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.1290 - accuracy: 0.9593 - val_loss: 0.3868 - val_accuracy: 0.8940\n",
            "Epoch 86/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1271 - accuracy: 0.9591\n",
            "Epoch 86: val_accuracy improved from 0.89950 to 0.90490, saving model to best_model.h5\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1271 - accuracy: 0.9591 - val_loss: 0.3527 - val_accuracy: 0.9049\n",
            "Epoch 87/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9597\n",
            "Epoch 87: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.1293 - accuracy: 0.9597 - val_loss: 0.4227 - val_accuracy: 0.8838\n",
            "Epoch 88/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9583\n",
            "Epoch 88: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 86ms/step - loss: 0.1318 - accuracy: 0.9583 - val_loss: 0.3899 - val_accuracy: 0.8965\n",
            "Epoch 89/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1321 - accuracy: 0.9594\n",
            "Epoch 89: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 39s 100ms/step - loss: 0.1321 - accuracy: 0.9594 - val_loss: 0.4520 - val_accuracy: 0.8828\n",
            "Epoch 90/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9582\n",
            "Epoch 90: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1293 - accuracy: 0.9582 - val_loss: 0.4123 - val_accuracy: 0.8885\n",
            "Epoch 91/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9623\n",
            "Epoch 91: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1205 - accuracy: 0.9623 - val_loss: 0.4659 - val_accuracy: 0.8888\n",
            "Epoch 92/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9632\n",
            "Epoch 92: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1193 - accuracy: 0.9632 - val_loss: 0.4314 - val_accuracy: 0.8932\n",
            "Epoch 93/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9605\n",
            "Epoch 93: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.1277 - accuracy: 0.9605 - val_loss: 0.5322 - val_accuracy: 0.8635\n",
            "Epoch 94/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9622\n",
            "Epoch 94: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1234 - accuracy: 0.9622 - val_loss: 0.4371 - val_accuracy: 0.8921\n",
            "Epoch 95/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9616\n",
            "Epoch 95: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.1219 - accuracy: 0.9616 - val_loss: 0.3814 - val_accuracy: 0.8992\n",
            "Epoch 96/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9624\n",
            "Epoch 96: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.1204 - accuracy: 0.9624 - val_loss: 0.4639 - val_accuracy: 0.8787\n",
            "Epoch 97/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9636\n",
            "Epoch 97: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1169 - accuracy: 0.9636 - val_loss: 0.3540 - val_accuracy: 0.9043\n",
            "Epoch 98/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9637\n",
            "Epoch 98: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1209 - accuracy: 0.9637 - val_loss: 0.3812 - val_accuracy: 0.8977\n",
            "Epoch 99/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9636\n",
            "Epoch 99: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1159 - accuracy: 0.9636 - val_loss: 0.3828 - val_accuracy: 0.9031\n",
            "Epoch 100/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1162 - accuracy: 0.9641\n",
            "Epoch 100: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.1162 - accuracy: 0.9641 - val_loss: 0.3925 - val_accuracy: 0.9009\n",
            "Epoch 101/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9649\n",
            "Epoch 101: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1118 - accuracy: 0.9649 - val_loss: 0.3691 - val_accuracy: 0.8974\n",
            "Epoch 102/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9642\n",
            "Epoch 102: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1132 - accuracy: 0.9642 - val_loss: 0.4402 - val_accuracy: 0.8941\n",
            "Epoch 103/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9661\n",
            "Epoch 103: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1104 - accuracy: 0.9661 - val_loss: 0.4172 - val_accuracy: 0.8942\n",
            "Epoch 104/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9667\n",
            "Epoch 104: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1101 - accuracy: 0.9667 - val_loss: 0.5138 - val_accuracy: 0.8787\n",
            "Epoch 105/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9654\n",
            "Epoch 105: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1102 - accuracy: 0.9654 - val_loss: 0.4267 - val_accuracy: 0.8939\n",
            "Epoch 106/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9658\n",
            "Epoch 106: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1092 - accuracy: 0.9658 - val_loss: 0.3937 - val_accuracy: 0.9012\n",
            "Epoch 107/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9661\n",
            "Epoch 107: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1056 - accuracy: 0.9661 - val_loss: 0.4475 - val_accuracy: 0.8943\n",
            "Epoch 108/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9667\n",
            "Epoch 108: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1085 - accuracy: 0.9667 - val_loss: 0.4470 - val_accuracy: 0.8912\n",
            "Epoch 109/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9665\n",
            "Epoch 109: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.1071 - accuracy: 0.9665 - val_loss: 0.4096 - val_accuracy: 0.8954\n",
            "Epoch 110/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9693\n",
            "Epoch 110: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0994 - accuracy: 0.9693 - val_loss: 0.4283 - val_accuracy: 0.8999\n",
            "Epoch 111/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9673\n",
            "Epoch 111: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1049 - accuracy: 0.9673 - val_loss: 0.4602 - val_accuracy: 0.8897\n",
            "Epoch 112/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9666\n",
            "Epoch 112: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1029 - accuracy: 0.9666 - val_loss: 0.4391 - val_accuracy: 0.8886\n",
            "Epoch 113/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9670\n",
            "Epoch 113: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1077 - accuracy: 0.9670 - val_loss: 0.4034 - val_accuracy: 0.8959\n",
            "Epoch 114/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9668\n",
            "Epoch 114: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.1020 - accuracy: 0.9668 - val_loss: 0.4066 - val_accuracy: 0.9010\n",
            "Epoch 115/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9699\n",
            "Epoch 115: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0929 - accuracy: 0.9699 - val_loss: 0.4282 - val_accuracy: 0.9004\n",
            "Epoch 116/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9686\n",
            "Epoch 116: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0993 - accuracy: 0.9686 - val_loss: 0.4686 - val_accuracy: 0.8882\n",
            "Epoch 117/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9671\n",
            "Epoch 117: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.1075 - accuracy: 0.9671 - val_loss: 0.4079 - val_accuracy: 0.8973\n",
            "Epoch 118/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9683\n",
            "Epoch 118: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1004 - accuracy: 0.9683 - val_loss: 0.4347 - val_accuracy: 0.8939\n",
            "Epoch 119/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9681\n",
            "Epoch 119: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.1023 - accuracy: 0.9681 - val_loss: 0.4232 - val_accuracy: 0.8938\n",
            "Epoch 120/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9708\n",
            "Epoch 120: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0944 - accuracy: 0.9708 - val_loss: 0.4143 - val_accuracy: 0.9018\n",
            "Epoch 121/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9705\n",
            "Epoch 121: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0943 - accuracy: 0.9705 - val_loss: 0.4108 - val_accuracy: 0.9040\n",
            "Epoch 122/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9698\n",
            "Epoch 122: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0977 - accuracy: 0.9698 - val_loss: 0.5375 - val_accuracy: 0.8795\n",
            "Epoch 123/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9689\n",
            "Epoch 123: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.0999 - accuracy: 0.9689 - val_loss: 0.4035 - val_accuracy: 0.8952\n",
            "Epoch 124/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9679\n",
            "Epoch 124: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1014 - accuracy: 0.9679 - val_loss: 0.3733 - val_accuracy: 0.9019\n",
            "Epoch 125/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9703\n",
            "Epoch 125: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0931 - accuracy: 0.9703 - val_loss: 0.3847 - val_accuracy: 0.9010\n",
            "Epoch 126/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9729\n",
            "Epoch 126: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.0878 - accuracy: 0.9729 - val_loss: 0.3968 - val_accuracy: 0.9008\n",
            "Epoch 127/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9722\n",
            "Epoch 127: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 79ms/step - loss: 0.0890 - accuracy: 0.9722 - val_loss: 0.4262 - val_accuracy: 0.8970\n",
            "Epoch 128/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9715\n",
            "Epoch 128: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0936 - accuracy: 0.9715 - val_loss: 0.3824 - val_accuracy: 0.8981\n",
            "Epoch 129/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9729\n",
            "Epoch 129: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0869 - accuracy: 0.9729 - val_loss: 0.4073 - val_accuracy: 0.8957\n",
            "Epoch 130/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9692\n",
            "Epoch 130: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.1028 - accuracy: 0.9692 - val_loss: 0.3881 - val_accuracy: 0.9044\n",
            "Epoch 131/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9709\n",
            "Epoch 131: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0953 - accuracy: 0.9709 - val_loss: 0.3958 - val_accuracy: 0.8945\n",
            "Epoch 132/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9724\n",
            "Epoch 132: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.0898 - accuracy: 0.9724 - val_loss: 0.3706 - val_accuracy: 0.9039\n",
            "Epoch 133/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9725\n",
            "Epoch 133: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0876 - accuracy: 0.9725 - val_loss: 0.4000 - val_accuracy: 0.9006\n",
            "Epoch 134/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9708\n",
            "Epoch 134: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0915 - accuracy: 0.9708 - val_loss: 0.4407 - val_accuracy: 0.8891\n",
            "Epoch 135/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9730\n",
            "Epoch 135: val_accuracy did not improve from 0.90490\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0871 - accuracy: 0.9730 - val_loss: 0.4112 - val_accuracy: 0.8990\n",
            "Epoch 136/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9738\n",
            "Epoch 136: val_accuracy improved from 0.90490 to 0.90670, saving model to best_model.h5\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0851 - accuracy: 0.9738 - val_loss: 0.3927 - val_accuracy: 0.9067\n",
            "Epoch 137/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9733\n",
            "Epoch 137: val_accuracy improved from 0.90670 to 0.90870, saving model to best_model.h5\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0878 - accuracy: 0.9733 - val_loss: 0.3899 - val_accuracy: 0.9087\n",
            "Epoch 138/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9723\n",
            "Epoch 138: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0869 - accuracy: 0.9723 - val_loss: 0.4835 - val_accuracy: 0.8870\n",
            "Epoch 139/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9728\n",
            "Epoch 139: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0888 - accuracy: 0.9728 - val_loss: 0.4428 - val_accuracy: 0.8902\n",
            "Epoch 140/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9738\n",
            "Epoch 140: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0851 - accuracy: 0.9738 - val_loss: 0.4554 - val_accuracy: 0.8951\n",
            "Epoch 141/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9746\n",
            "Epoch 141: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0831 - accuracy: 0.9746 - val_loss: 0.4196 - val_accuracy: 0.9019\n",
            "Epoch 142/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9737\n",
            "Epoch 142: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0839 - accuracy: 0.9737 - val_loss: 0.4263 - val_accuracy: 0.8984\n",
            "Epoch 143/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9716\n",
            "Epoch 143: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0928 - accuracy: 0.9716 - val_loss: 0.3945 - val_accuracy: 0.9041\n",
            "Epoch 144/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9758\n",
            "Epoch 144: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0796 - accuracy: 0.9758 - val_loss: 0.4332 - val_accuracy: 0.8978\n",
            "Epoch 145/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9746\n",
            "Epoch 145: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0821 - accuracy: 0.9746 - val_loss: 0.4906 - val_accuracy: 0.8871\n",
            "Epoch 146/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9744\n",
            "Epoch 146: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0803 - accuracy: 0.9744 - val_loss: 0.4210 - val_accuracy: 0.9074\n",
            "Epoch 147/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9746\n",
            "Epoch 147: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0832 - accuracy: 0.9746 - val_loss: 0.4169 - val_accuracy: 0.9045\n",
            "Epoch 148/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.9745\n",
            "Epoch 148: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0820 - accuracy: 0.9745 - val_loss: 0.4216 - val_accuracy: 0.9042\n",
            "Epoch 149/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9752\n",
            "Epoch 149: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0839 - accuracy: 0.9752 - val_loss: 0.4190 - val_accuracy: 0.9034\n",
            "Epoch 150/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9741\n",
            "Epoch 150: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0841 - accuracy: 0.9741 - val_loss: 0.4309 - val_accuracy: 0.8957\n",
            "Epoch 151/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9757\n",
            "Epoch 151: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0788 - accuracy: 0.9757 - val_loss: 0.5282 - val_accuracy: 0.8920\n",
            "Epoch 152/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9761\n",
            "Epoch 152: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0772 - accuracy: 0.9761 - val_loss: 0.4553 - val_accuracy: 0.8977\n",
            "Epoch 153/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9750\n",
            "Epoch 153: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0825 - accuracy: 0.9750 - val_loss: 0.5029 - val_accuracy: 0.8977\n",
            "Epoch 154/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9733\n",
            "Epoch 154: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0874 - accuracy: 0.9733 - val_loss: 0.4460 - val_accuracy: 0.8858\n",
            "Epoch 155/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0894 - accuracy: 0.9741\n",
            "Epoch 155: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0894 - accuracy: 0.9741 - val_loss: 0.3932 - val_accuracy: 0.9029\n",
            "Epoch 156/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9768\n",
            "Epoch 156: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0774 - accuracy: 0.9768 - val_loss: 0.4117 - val_accuracy: 0.9046\n",
            "Epoch 157/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9751\n",
            "Epoch 157: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 80ms/step - loss: 0.0818 - accuracy: 0.9751 - val_loss: 0.4707 - val_accuracy: 0.8985\n",
            "Epoch 158/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9745\n",
            "Epoch 158: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0819 - accuracy: 0.9745 - val_loss: 0.4295 - val_accuracy: 0.8994\n",
            "Epoch 159/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9774\n",
            "Epoch 159: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0761 - accuracy: 0.9774 - val_loss: 0.4284 - val_accuracy: 0.9011\n",
            "Epoch 160/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9771\n",
            "Epoch 160: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0740 - accuracy: 0.9771 - val_loss: 0.4861 - val_accuracy: 0.8933\n",
            "Epoch 161/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9763\n",
            "Epoch 161: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0791 - accuracy: 0.9763 - val_loss: 0.4074 - val_accuracy: 0.9048\n",
            "Epoch 162/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9781\n",
            "Epoch 162: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0749 - accuracy: 0.9781 - val_loss: 0.4621 - val_accuracy: 0.8969\n",
            "Epoch 163/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9728\n",
            "Epoch 163: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0891 - accuracy: 0.9728 - val_loss: 0.4220 - val_accuracy: 0.8985\n",
            "Epoch 164/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9747\n",
            "Epoch 164: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.0799 - accuracy: 0.9747 - val_loss: 0.4314 - val_accuracy: 0.8981\n",
            "Epoch 165/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n",
            "Epoch 165: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.4361 - val_accuracy: 0.8998\n",
            "Epoch 166/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9752\n",
            "Epoch 166: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.0804 - accuracy: 0.9752 - val_loss: 0.3917 - val_accuracy: 0.9046\n",
            "Epoch 167/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9773\n",
            "Epoch 167: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 31s 81ms/step - loss: 0.0739 - accuracy: 0.9773 - val_loss: 0.4457 - val_accuracy: 0.9005\n",
            "Epoch 168/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9771\n",
            "Epoch 168: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0733 - accuracy: 0.9771 - val_loss: 0.4324 - val_accuracy: 0.8999\n",
            "Epoch 169/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9779\n",
            "Epoch 169: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0745 - accuracy: 0.9779 - val_loss: 0.4821 - val_accuracy: 0.8890\n",
            "Epoch 170/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9773\n",
            "Epoch 170: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0745 - accuracy: 0.9773 - val_loss: 0.4437 - val_accuracy: 0.8989\n",
            "Epoch 171/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9776\n",
            "Epoch 171: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 0.4438 - val_accuracy: 0.8930\n",
            "Epoch 172/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9773\n",
            "Epoch 172: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.0715 - accuracy: 0.9773 - val_loss: 0.4103 - val_accuracy: 0.9073\n",
            "Epoch 173/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9776\n",
            "Epoch 173: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 0.4223 - val_accuracy: 0.9061\n",
            "Epoch 174/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9754\n",
            "Epoch 174: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0826 - accuracy: 0.9754 - val_loss: 0.4450 - val_accuracy: 0.8986\n",
            "Epoch 175/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9776\n",
            "Epoch 175: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0730 - accuracy: 0.9776 - val_loss: 0.4890 - val_accuracy: 0.8983\n",
            "Epoch 176/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9769\n",
            "Epoch 176: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0761 - accuracy: 0.9769 - val_loss: 0.4667 - val_accuracy: 0.8963\n",
            "Epoch 177/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9782\n",
            "Epoch 177: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0749 - accuracy: 0.9782 - val_loss: 0.5661 - val_accuracy: 0.8840\n",
            "Epoch 178/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9785\n",
            "Epoch 178: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.0689 - accuracy: 0.9785 - val_loss: 0.4259 - val_accuracy: 0.9081\n",
            "Epoch 179/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9787\n",
            "Epoch 179: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 82ms/step - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.4340 - val_accuracy: 0.9061\n",
            "Epoch 180/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9777\n",
            "Epoch 180: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 83ms/step - loss: 0.0752 - accuracy: 0.9777 - val_loss: 0.4521 - val_accuracy: 0.9057\n",
            "Epoch 181/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9775\n",
            "Epoch 181: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0727 - accuracy: 0.9775 - val_loss: 0.4641 - val_accuracy: 0.8910\n",
            "Epoch 182/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9791\n",
            "Epoch 182: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 83ms/step - loss: 0.0695 - accuracy: 0.9791 - val_loss: 0.4947 - val_accuracy: 0.9005\n",
            "Epoch 183/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9780\n",
            "Epoch 183: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0736 - accuracy: 0.9780 - val_loss: 0.4941 - val_accuracy: 0.8982\n",
            "Epoch 184/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9781\n",
            "Epoch 184: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 85ms/step - loss: 0.0758 - accuracy: 0.9781 - val_loss: 0.4178 - val_accuracy: 0.9020\n",
            "Epoch 185/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9784\n",
            "Epoch 185: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0722 - accuracy: 0.9784 - val_loss: 0.5094 - val_accuracy: 0.8926\n",
            "Epoch 186/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9792\n",
            "Epoch 186: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 33s 84ms/step - loss: 0.0666 - accuracy: 0.9792 - val_loss: 0.5611 - val_accuracy: 0.8848\n",
            "Epoch 187/500\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9792\n",
            "Epoch 187: val_accuracy did not improve from 0.90870\n",
            "390/390 [==============================] - 32s 81ms/step - loss: 0.0693 - accuracy: 0.9792 - val_loss: 0.4873 - val_accuracy: 0.8988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "ZPmzC7pdF2q6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "f3a0bfa9-d29e-4813-a1e1-0ba6e0d61ffc"
      },
      "id": "ZPmzC7pdF2q6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.3899 - accuracy: 0.9087 - 1s/epoch - 4ms/step\n",
            "0.9086999893188477\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_from_file = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "test_loss, test_acc = model_from_file.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffSGXroJmYOu",
        "outputId": "c8994e14-7eca-48cf-905c-7962b4a2f868"
      },
      "id": "ffSGXroJmYOu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.3899 - accuracy: 0.9087 - 1s/epoch - 4ms/step\n",
            "0.9086999893188477\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}